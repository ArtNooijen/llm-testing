# LLM Testing Interface

A Python interface for testing different LLM inference backends with a focus on Ollama. This project provides a swappable interface architecture that allows you to easily switch between different inference backends.

## Features

- ðŸš€ **Interactive CLI**: Beautiful command-line interface with rich formatting
- ðŸ”„ **Swappable Backends**: Easy to add new inference backends
- ðŸŒ **Remote Connection**: Connect to Ollama running on remote machines via Tailscale
- âš™ï¸ **Configurable**: YAML-based configuration
- ðŸ“¦ **UV Managed**: Modern Python dependency management

## Prerequisites

- Python 3.10+
- [UV](https://docs.astral.sh/uv/) package manager
- Ollama running on a remote machine (accessible via Tailscale)
- Tailscale network access to the remote machine

## Installation

1. **Install UV** (if not already installed):
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```

2. **Clone and setup the project**:
   ```bash
   cd llm-testing
   uv sync
   ```

## Configuration

The project uses `config.yaml` for configuration. The default configuration connects to:

- **Host**: 100.70.84.114 (drutus via Tailscale)
- **Port**: 11434 (default Ollama port)
- **Model**: mistral:7b

### Customizing Configuration

Edit `config.yaml` to change settings:

```yaml
# Ollama Configuration
ollama:
  host: "100.70.84.114"  # Your remote machine IP
  port: 11434
  model: "mistral:7b"     # Model to use
  timeout: 30            # Request timeout in seconds

# CLI Configuration
cli:
  welcome_message: "Welcome to LLM Testing Interface!"
  prompt: "You: "
  exit_commands: ["/exit", "/quit", "/q"]
  clear_commands: ["/clear", "/cls"]
```

## Usage

### Running the Interface

```bash
uv run python src/main.py
```

### Commands

- **Type your message**: Just type and press Enter to send a message
- **`/exit`**, **`/quit`**, **`/q`**: Exit the application
- **`/clear`**, **`/cls`**: Clear the screen
- **Ctrl+C**: Force exit

### Example Session

```
â”Œâ”€ LLM Testing Interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Welcome to LLM Testing Interface!                              â”‚
â”‚                                                                â”‚
â”‚ Status: âœ“ Connected to mistral:7b                            â”‚
â”‚                                                                â”‚
â”‚ Host: 100.70.84.114:11434                                     â”‚
â”‚ Model: mistral:7b                                             â”‚
â”‚                                                                â”‚
â”‚ Commands: /exit to quit, /clear to clear screen              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

You: Hello, how are you today?

Thinking...

Assistant:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Hello! I'm doing well, thank you for asking. I'm here and     â”‚
â”‚ ready to help you with any questions or tasks you might have. â”‚
â”‚ How can I assist you today?                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Architecture

### Interface System

The project uses an abstract base class pattern for easy backend swapping:

```
src/
â”œâ”€â”€ interfaces/
â”‚   â”œâ”€â”€ base.py          # Abstract base interface
â”‚   â””â”€â”€ ollama.py        # Ollama implementation
â”œâ”€â”€ config.py            # Configuration management
â””â”€â”€ main.py             # Interactive CLI
```

### Adding New Backends

To add a new inference backend:

1. **Create a new interface class** in `src/interfaces/`:

```python
from .base import BaseInferenceInterface

class MyBackendInterface(BaseInferenceInterface):
    def generate(self, prompt: str) -> str:
        # Implementation here
        pass
    
    def is_available(self) -> bool:
        # Check availability
        pass
    
    def get_model_info(self) -> Optional[dict]:
        # Return model info
        pass
```

2. **Update the main application** to use your new backend
3. **Add configuration** for your backend in `config.yaml`

## Troubleshooting

### Connection Issues

1. **Check Tailscale connection**:
   ```bash
   ping 100.70.84.114
   ```

2. **Verify Ollama is running** on the remote machine:
   ```bash
   curl http://100.70.84.114:11434/api/tags
   ```

3. **Check firewall settings** on the remote machine

### Common Issues

- **"Cannot connect to Ollama"**: Verify the remote machine is accessible and Ollama is running
- **"Model not found"**: Ensure the model is pulled on the remote Ollama instance
- **Timeout errors**: Increase the timeout value in `config.yaml`

## Development

### Project Structure

```
llm-testing/
â”œâ”€â”€ pyproject.toml          # UV project configuration
â”œâ”€â”€ config.yaml            # Runtime configuration
â”œâ”€â”€ .gitignore             # Git ignore rules
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py            # Interactive CLI entry point
â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â””â”€â”€ interfaces/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base.py        # Abstract base interface
â”‚       â””â”€â”€ ollama.py      # Ollama implementation
â””â”€â”€ README.md              # This file
```

### Dependencies

- `ollama`: Ollama Python client
- `pyyaml`: YAML configuration parsing
- `rich`: Beautiful terminal output

## License

This project is open source and available under the MIT License.
