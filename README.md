# LLM Testing Interface

A Python interface for testing and comparing different LLM models across multiple Ollama instances. This project provides a swappable interface architecture that allows you to easily test models from different sources and compare their responses.

## Features

- ðŸš€ **Model Comparison Mode**: Test multiple models with the same prompt and compare responses
- ðŸ”„ **Multi-Instance Support**: Connect to multiple Ollama instances (remote and local)
- ðŸŒ **Remote Connection**: Connect to Ollama running on remote machines via Tailscale
- âš™ï¸ **Configurable**: YAML-based configuration for models and instances
- ðŸ“¦ **UV Managed**: Modern Python dependency management
- ðŸŽ¯ **Single Prompt Testing**: Enter once, test all configured models

## Prerequisites

- Python 3.10+
- [UV](https://docs.astral.sh/uv/) package manager
- Ollama running on one or more machines:
  - Remote machine (accessible via Tailscale) - optional
  - Local machine (localhost) - optional
- Tailscale network access (if using remote Ollama)

## Installation

1. **Install UV** (if not already installed):
   ```bash
   curl -LsSf https://astral.sh/uv/install.sh | sh
   ```

2. **Clone and setup the project**:
   ```bash
   cd llm-testing
   uv sync
   ```

## Configuration

The project uses `config.yaml` for configuration. You can configure multiple Ollama instances and specify which models run on which instance.

### Default Configuration

```yaml
# Ollama Instances
ollama_instances:
  remote:
    host: "drutus"  # drutus via Tailscale
    port: 11434
    timeout: 30
  local:
    host: "localhost"  # local mac run ollama
    port: 11434
    timeout: 30

# Models to test (specify which instance each model runs on)
models:
  - name: "mistral:7b"
    instance: "remote"
  - name: "llama3.1:8b"
    instance: "local"
  - name: "codellama:7b"
    instance: "remote"

# CLI Configuration
cli:
  welcome_message: "Welcome to LLM Testing Interface!"
  prompt: "Enter your prompt: "
  comparison_mode: true  # Run all models and compare responses
```

### Customizing Configuration

**Add more models:**
```yaml
models:
  - name: "mistral:7b"
    instance: "remote"
  - name: "llama3.1:8b"
    instance: "local"
  - name: "phi:2.7b"
    instance: "local"
  - name: "codellama:7b"
    instance: "remote"
```

**Add more Ollama instances:**
```yaml
ollama_instances:
  remote:
    host: "drutus"
    port: 11434
    timeout: 30
  local:
    host: "localhost"
    port: 11434
    timeout: 30
  cloud:
    host: "your-cloud-instance.com"
    port: 11434
    timeout: 60
```

## Usage

### Running the Interface

```bash
uv run python src/main.py
```

### Comparison Mode (Default)

The application runs in comparison mode by default. You enter a single prompt and it tests all configured models:

```
â”Œâ”€ LLM Testing Interface â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Welcome to LLM Testing Interface!                              â”‚
â”‚                                                                â”‚
â”‚ Status: âœ“ Connected                                            â”‚
â”‚                                                                â”‚
â”‚ Host: drutus:11434, localhost:11434                           â”‚
â”‚ Models: mistral:7b, llama3.1:8b, codellama:7b                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Enter your prompt: Explain quantum computing in simple terms

Testing 3 models with the same prompt...
Prompt: Explain quantum computing in simple terms

Testing Model 1/3: mistral:7b (on remote)
Thinking...

Testing Model 2/3: llama3.1:8b (on local)
Thinking...

Testing Model 3/3: codellama:7b (on remote)
Thinking...

=== COMPARISON RESULTS ===

â”Œâ”€ Model 1: mistral:7b (remote) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Quantum computing is like having a computer that can be in     â”‚
â”‚ multiple states at once, unlike regular computers that are     â”‚
â”‚ either on or off...                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Model 2: llama3.1:8b (local) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Think of quantum computing like having a coin that can be       â”‚
â”‚ both heads and tails at the same time until you look at it...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Model 3: codellama:7b (remote) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Quantum computing uses quantum bits (qubits) that can exist     â”‚
â”‚ in superposition, allowing for parallel processing...         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Interactive Mode

To switch to interactive mode (chat with one model), set in `config.yaml`:

```yaml
cli:
  comparison_mode: false
```

### Commands (Interactive Mode)

- **Type your message**: Just type and press Enter to send a message
- **`/exit`**, **`/quit`**, **`/q`**: Exit the application
- **`/clear`**, **`/cls`**: Clear the screen
- **Ctrl+C**: Force exit

## Architecture

### Interface System

The project uses an abstract base class pattern for easy backend swapping:

```
src/
â”œâ”€â”€ interfaces/
â”‚   â”œâ”€â”€ base.py          # Abstract base interface
â”‚   â””â”€â”€ ollama.py        # Ollama implementation
â”œâ”€â”€ config.py            # Configuration management
â””â”€â”€ main.py             # Interactive CLI
```

### Adding New Backends

To add a new inference backend:

1. **Create a new interface class** in `src/interfaces/`:

```python
from .base import BaseInferenceInterface

class MyBackendInterface(BaseInferenceInterface):
    def generate(self, prompt: str) -> str:
        # Implementation here
        pass
    
    def is_available(self) -> bool:
        # Check availability
        pass
    
    def get_model_info(self) -> Optional[dict]:
        # Return model info
        pass
```

2. **Update the main application** to use your new backend
3. **Add configuration** for your backend in `config.yaml`

## Troubleshooting

### Connection Issues

1. **Check Tailscale connection**:
   ```bash
   ping 100.70.84.114
   ```

2. **Verify Ollama is running** on the remote machine:
   ```bash
   curl http://100.70.84.114:11434/api/tags
   ```

3. **Check firewall settings** on the remote machine

### Common Issues

- **"Cannot connect to Ollama"**: Verify the remote machine is accessible and Ollama is running
- **"Model not found"**: Ensure the model is pulled on the remote Ollama instance
- **Timeout errors**: Increase the timeout value in `config.yaml`

## Development

### Project Structure

```
llm-testing/
â”œâ”€â”€ pyproject.toml          # UV project configuration
â”œâ”€â”€ config.yaml            # Runtime configuration
â”œâ”€â”€ .gitignore             # Git ignore rules
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py            # Main application (comparison & interactive modes)
â”‚   â”œâ”€â”€ config.py          # Configuration management
â”‚   â””â”€â”€ interfaces/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base.py        # Abstract base interface
â”‚       â””â”€â”€ ollama.py      # Ollama implementation
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_connection.py # Connection testing utilities
â””â”€â”€ README.md              # This file
```

### Dependencies

- `ollama`: Ollama Python client
- `pyyaml`: YAML configuration parsing
- `rich`: Beautiful terminal output
- `requests`: HTTP client for connection testing

### Key Features

- **Multi-Instance Support**: Test models across different Ollama instances
- **Model Comparison**: Side-by-side comparison of model responses
- **Flexible Configuration**: Easy to add new models and instances
- **Error Handling**: Graceful handling of connection and model errors
- **Rich Output**: Beautiful terminal formatting with progress indicators

## License

This project is open source and available under the MIT License.
